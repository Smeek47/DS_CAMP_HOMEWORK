{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hometask\n",
    "\n",
    "1) Find text to train (any book)<br>\n",
    "2) Build train and validation set <br>\n",
    "3) Train bidirectional language model that predicts the POS of word being based on its `n_context= 3` neighbours from the left and `n_context= 3` neighbours from the right <br>\n",
    "4) Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution \n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk import pos_tag\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry potter and the sorcerer’s stone for jessica, who loves stories, for anne, who loved them too; and for di, who heard this one first. . the boy who lived mr. and mrs. dursley, of number four, privet drive, were proud to say that they were perfectly normal, thank you very much. they were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. mr. dursley was the director of a firm called grunnings, which made drills. he '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)  # Remove text within square brackets\n",
    "        text = re.sub(r'\\d+', '', text)      # Remove digits\n",
    "        text = re.sub(r'«|»', '', text)      # Remove special characters\n",
    "        text = re.sub(r'\\n', ' ', text)      # Replace newline characters with space\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Load and preprocess the text data\n",
    "text = preprocess_text('data/Rouling_Harry_Potter_1_Harry_Potter_and_the_Sorcerers_Stone.txt')\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the raw text and prepare the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "n_context = 3\n",
    "\n",
    "for i in range(n_context, len(tagged) - n_context):\n",
    "    X.append([word for word, _ in tagged[i-n_context:i+n_context+1]])\n",
    "    y.append(tagged[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train and validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words and tags into numerical IDs\n",
    "word2idx = {word: i for i, word in enumerate(set(word for word_list in X_train for word in word_list))}\n",
    "tag2idx = {tag: i for i, tag in enumerate(set(y_train))}\n",
    "\n",
    "# Update the sequences with the numerical IDs\n",
    "X_train = [[word2idx[word] for word in word_list] for word_list in X_train]\n",
    "X_val = [[word2idx[word] for word in word_list] for word_list in X_val]\n",
    "y_train = [tag2idx[tag] for tag in y_train]\n",
    "y_val = [tag2idx[tag] for tag in y_val]\n",
    "\n",
    "# Pad the sequences\n",
    "X_train = pad_sequences(X_train, maxlen=n_context*2+1)\n",
    "X_val = pad_sequences(X_val, maxlen=n_context*2+1)\n",
    "\n",
    "# One-hot encode\n",
    "y_train = to_categorical(y_train, num_classes=len(tag2idx))\n",
    "y_val = to_categorical(y_val, num_classes=len(tag2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (79354, 7)\n",
      "Shape of y_train: (79354, 38)\n",
      "Shape of X_val: (19839, 7)\n",
      "Shape of y_val: (19839, 38)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2idx), output_dim=64, input_length=n_context*2+1))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dense(len(tag2idx), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2480/2480 [==============================] - 42s 15ms/step - loss: 1.4852 - accuracy: 0.5628 - val_loss: 0.6186 - val_accuracy: 0.8219\n",
      "Epoch 2/10\n",
      "2480/2480 [==============================] - 36s 14ms/step - loss: 0.4281 - accuracy: 0.8731 - val_loss: 0.4417 - val_accuracy: 0.8702\n",
      "Epoch 3/10\n",
      "2480/2480 [==============================] - 36s 15ms/step - loss: 0.2634 - accuracy: 0.9190 - val_loss: 0.4275 - val_accuracy: 0.8760\n",
      "Epoch 4/10\n",
      "2480/2480 [==============================] - 41s 16ms/step - loss: 0.1923 - accuracy: 0.9392 - val_loss: 0.4277 - val_accuracy: 0.8770\n",
      "Epoch 5/10\n",
      "2480/2480 [==============================] - 41s 17ms/step - loss: 0.1465 - accuracy: 0.9538 - val_loss: 0.4592 - val_accuracy: 0.8767\n",
      "Epoch 6/10\n",
      "2480/2480 [==============================] - 38s 15ms/step - loss: 0.1178 - accuracy: 0.9618 - val_loss: 0.4655 - val_accuracy: 0.8814\n",
      "Epoch 7/10\n",
      "2480/2480 [==============================] - 38s 15ms/step - loss: 0.0943 - accuracy: 0.9696 - val_loss: 0.5241 - val_accuracy: 0.8769\n",
      "Epoch 8/10\n",
      "2480/2480 [==============================] - 42s 17ms/step - loss: 0.0780 - accuracy: 0.9748 - val_loss: 0.5200 - val_accuracy: 0.8795\n",
      "Epoch 9/10\n",
      "2480/2480 [==============================] - 42s 17ms/step - loss: 0.0634 - accuracy: 0.9796 - val_loss: 0.5551 - val_accuracy: 0.8775\n",
      "Epoch 10/10\n",
      "2480/2480 [==============================] - 40s 16ms/step - loss: 0.0543 - accuracy: 0.9821 - val_loss: 0.5860 - val_accuracy: 0.8777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fad31d6880>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 3s 4ms/step - loss: 0.5860 - accuracy: 0.8777\n",
      "Validation Loss: 0.5859730839729309\n",
      "Validation Accuracy: 0.8776652216911316\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 4s 4ms/step\n",
      "Sample: [and | daddy. | ” | “ | all | right | ,]\n",
      "True word: “\n",
      "True tag: NNP\n",
      "Predicted tag: NNP\n",
      "\n",
      "Sample: [was | carrying | a | large | wooden | crate | under]\n",
      "True word: large\n",
      "True tag: JJ\n",
      "Predicted tag: JJ\n",
      "\n",
      "Sample: [gives | you | the | right | to | walk | around]\n",
      "True word: right\n",
      "True tag: NN\n",
      "Predicted tag: NN\n",
      "\n",
      "Sample: [movement | we | ’ | ve | been | practicing | !]\n",
      "True word: ve\n",
      "True tag: RB\n",
      "Predicted tag: RB\n",
      "\n",
      "Sample: [if | they | could | make | a | pineapple | tapdance]\n",
      "True word: make\n",
      "True tag: VB\n",
      "Predicted tag: VB\n",
      "\n",
      "Sample: [together | . | “ | i | ’ | d | not]\n",
      "True word: i\n",
      "True tag: JJ\n",
      "Predicted tag: NN\n",
      "\n",
      "Sample: [the | floor | in | fright | ; | ron | pulled]\n",
      "True word: fright\n",
      "True tag: NN\n",
      "Predicted tag: NN\n",
      "\n",
      "Sample: [each | other | , | seeming | to | have | forgotten]\n",
      "True word: seeming\n",
      "True tag: VBG\n",
      "Predicted tag: RB\n",
      "\n",
      "Sample: [lessons | , | we | ’ | ll | get | into]\n",
      "True word: ’\n",
      "True tag: VBP\n",
      "Predicted tag: VBP\n",
      "\n",
      "Sample: [could | remember | , | ever | since | he | ’]\n",
      "True word: ever\n",
      "True tag: RB\n",
      "Predicted tag: RB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After training the model, make predictions on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "\n",
    "# Choose 10 random samples from the validation set\n",
    "indices = np.random.choice(range(len(X_val)), size=10, replace=False)\n",
    "\n",
    "for i in indices:\n",
    "    # Get the true and predicted tags\n",
    "    true_tag = np.argmax(y_val[i])\n",
    "    pred_tag = np.argmax(y_pred[i])\n",
    "\n",
    "    # Get the corresponding words\n",
    "    words = [idx2word[idx] for idx in X_val[i]]\n",
    "\n",
    "    print(f'Sample: [{\" | \".join(words)}]')\n",
    "    print(f'True word: {words[3]}')\n",
    "    print(f'True tag: {idx2tag[true_tag]}')\n",
    "    print(f'Predicted tag: {idx2tag[pred_tag]}')\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lesson28",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
